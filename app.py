# app.py
import os
import tempfile
from pathlib import Path

import streamlit as st
from dotenv import load_dotenv

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from sentence_transformers import SentenceTransformer
from langchain_huggingface import HuggingFaceEmbeddings


# -----------------------------
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# -----------------------------
# ê¸°ë³¸ ë¡œë“œ
load_dotenv()
# ë³´ì¡° íƒìƒ‰ (ì›í•˜ì‹œëŠ” ê²½ë¡œ ì¶”ê°€ ê°€ëŠ¥)
candidates = [
    Path(".env"),
    Path(__file__).parent / ".env",
    Path(__file__).parent.parent / ".env",
    Path(__file__).parent.parent / "poet" / ".env",
]
for p in candidates:
    if p.exists():
        load_dotenv(dotenv_path=p, override=True)

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# -----------------------------
# Streamlit UI
# -----------------------------
st.set_page_config(page_title="ChatPDF", page_icon="ğŸ“„")
st.title("ğŸ“„ ChatPDF (Upload â†’ Split â†’ Embed â†’ Ask)")
st.write("---")

# íŒŒì¼ ì—…ë¡œë“œ
upload_file = st.file_uploader("pdf íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”!", type=["pdf"])
st.write("---")

# ì„¸ì…˜ ìƒíƒœ í‚¤ ì´ˆê¸°í™”
for k in ["texts", "vectorstore", "retriever_ready"]:
    if k not in st.session_state:
        st.session_state[k] = None

# -----------------------------
# ìœ í‹¸: PDF -> Documents
# -----------------------------
def pdf_to_documents(upload):
    temp_dir = tempfile.TemporaryDirectory()
    temp_path = os.path.join(temp_dir.name, upload.name)
    with open(temp_path, "wb") as f:
        f.write(upload.getvalue())
    loader = PyPDFLoader(temp_path)
    # load()ëŠ” í˜ì´ì§€ ë‹¨ìœ„ Document ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    return loader.load()  # temp_dirëŠ” í•¨ìˆ˜ ëë‚˜ë„ íŒŒì¼ì€ ì—´ëŒ ê°€ëŠ¥(ë¡œë“œë¨)

# -----------------------------
# ë‹¨ê³„ 1) ì—…ë¡œë“œ í›„ ë¶„í• 
# -----------------------------
if upload_file is not None:
    try:
        pages = pdf_to_documents(upload_file)

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,
            chunk_overlap=20,
            length_function=len,
            is_separator_regex=False,
        )
        texts = splitter.split_documents(pages)
        st.session_state.texts = texts
        st.info(f"âœ… í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: ì´ {len(texts)}ê°œ ì²­í¬")

    except Exception as e:
        st.error(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {type(e).__name__}: {e}")
        st.stop()

# -----------------------------
# ë‹¨ê³„ 2) ì„ë² ë”© & ë²¡í„°DB êµ¬ì¶•
# -----------------------------
if st.session_state.texts is not None:
    if st.session_state.vectorstore is None:
        if st.button("ë²¡í„°í™”(ì„ë² ë”©) ì‹œì‘"):
            if not GOOGLE_API_KEY:
                st.error("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                st.stop()
            try:
                embeddings = HuggingFaceEmbeddings(model_name="upskyy/bge-m3-korean")
                
                # ë©”ëª¨ë¦¬ ë²¡í„°DB (í•„ìš” ì‹œ persist_directory ì§€ì •)
                st.session_state.vectorstore = Chroma.from_documents(
                    st.session_state.texts, embeddings
                )
                st.success("âœ… ì„ë² ë”© ë° ë²¡í„°DB êµ¬ì¶• ì™„ë£Œ")
            except Exception as e:
                st.error(f"ì„ë² ë”© ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜: {type(e).__name__}: {e}")

# -----------------------------
# ë‹¨ê³„ 3) RAG ì§ˆì˜
# -----------------------------
if st.session_state.vectorstore is not None:
    st.header("PDFì—ê²Œ ì§ˆë¬¸í•´ ë³´ì„¸ìš”!!")
    question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ) ì´ ë¬¸ì„œì—ì„œ ì„¤ì¹˜ ì ˆì°¨ë¥¼ ìš”ì•½í•´ì¤˜")

    # LLM ì¤€ë¹„ (ì§ˆì˜ ì‹œì ì— ìƒì„±)
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        google_api_key=GOOGLE_API_KEY,
        # temperature=0.2  # ì›í•˜ì‹œë©´ ë³´ìˆ˜ì ìœ¼ë¡œ ì¡°ì •
    )

    # MultiQueryRetriever: ì§ˆë¬¸ì„ ë‹¤ì–‘í•œ ê´€ì ìœ¼ë¡œ í™•ì¥í•´ ë” ì¢‹ì€ ë¬¸ì„œë¥¼ ê²€ìƒ‰
    base_retriever = st.session_state.vectorstore.as_retriever()

    # í”„ë¡¬í”„íŠ¸ (ë¡œì»¬ í…œí”Œë¦¿ ì‚¬ìš©: hub.pull ì˜ì¡´ ì œê±°)
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=(
            "ë‹¹ì‹ ì€ ì‹ ì¤‘í•˜ê³  ì‚¬ì‹¤ì— ê·¼ê±°í•œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.\n"
            "ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë§Œ ê·¼ê±°ë¡œ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µí•˜ì„¸ìš”. "
            "ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì„¸ìš”.\n\n"
            "ì»¨í…ìŠ¤íŠ¸:\n{context}\n\n"
            "ì§ˆë¬¸:\n{question}\n\n"
            "ë‹µë³€:"
        ),
    )

    def format_docs(docs):
        return "\n\n".join(d.page_content for d in docs)

    if st.button("ì§ˆë¬¸í•˜ê¸°"):
        if not question.strip():
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
            st.stop()

        try:
            mq_retriever = MultiQueryRetriever.from_llm(
                retriever=base_retriever,
                llm=llm,
            )

            # RAG ì²´ì¸
            rag_chain = (
                {
                    "context": mq_retriever | format_docs,
                    "question": RunnablePassthrough(),
                }
                | prompt
                | llm
                | StrOutputParser()
            )

            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                answer = rag_chain.invoke(question)

            st.subheader("ë‹µë³€")
            st.write(answer)

            # ì°¸ê³ ë¡œ ì‚¬ìš©ëœ ë¬¸ì„œ ì¡°ê°ë„ ë³´ì—¬ì£¼ê¸° (ìƒìœ„ 3ê°œ)
            with st.expander("ğŸ” ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸(ìƒìœ„ 3ê°œ ë³´ê¸°)"):
                ctx_docs = base_retriever.get_relevant_documents(question)[:3]
                for i, d in enumerate(ctx_docs, 1):
                    st.markdown(f"**[ì»¨í…ìŠ¤íŠ¸ {i}]**")
                    st.write(d.page_content[:1500] + ("..." if len(d.page_content) > 1500 else ""))
                    meta = d.metadata or {}
                    src = f"page={meta.get('page', 'N/A')}, source={meta.get('source', 'N/A')}"
                    st.caption(src)

        except Exception as e:
            st.error(f"ì§ˆì˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {type(e).__name__}: {e}")

# -----------------------------
# ì•ˆë‚´
# -----------------------------
st.write("---")
st.caption(
    "Tip: ì„ë² ë”©ì€ ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œ í•œ ë²ˆë§Œ ìˆ˜í–‰ë˜ë©°, ê°™ì€ ì„¸ì…˜ì—ì„œëŠ” ì¬ì‚¬ìš©ë©ë‹ˆë‹¤. "
    "ì˜¤ë¥˜ê°€ ê³„ì†ë˜ë©´ íŒ¨í‚¤ì§€ë¥¼ ì—…ë°ì´íŠ¸í•´ ë³´ì„¸ìš”: "
    "`pip install -U langchain langchain-community langchain-text-splitters "
    "langchain-google-genai google-generativeai chromadb`"
)